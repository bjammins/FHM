library(psychometric)
library(ltm)
library(eRm)
library(CTT)
library(reshape)
library(ggplot2)
library(plyr)
library(irr)
#setwd("L:/FHM/Statistical Analysis")
setwd("C:/Users/bscheich/Desktop")
#data<-read.csv("FHM_test_A_advanced.csv", header=TRUE)#dichotomous data
raw<-read.csv("FHM_raw_data.csv",header=TRUE)#original data
id<-raw$User.Num
raw<-raw[,4:53]#raw data without test taking identifiers
key<-read.csv("FHM_key.csv",header=FALSE)#key for raw data
###Score the data
score<-score(raw,key=key,output.scored=TRUE,ID=id,rel=TRUE) #score all items
cumulative.score<-score$score  #total score for each test taker
cumulative.score<-as.numeric(cumulative.score)
score.matrix<-score$scored #matrix of 1s and 0s of scored items
set.seed(50)
sample<-score.matrix[sample(nrow(score.matrix),size=1000,replace=FALSE,),]
rel<-score$reliability  #calculate reliability
cumulative.score<-as.data.frame(cumulative.score)
colnames(cumulative.score)[1]<-"score"
#Histogram of Advanced FHM
ggplot(cumulative.score,aes(x=score)) + geom_histogram(aes(y=..density..),binwidth=1, colour="black", fill="white")+labs(title="Advanced FHM Scores")+xlab("Total Score") +ylab("% of Total")+theme(plot.title = element_text(size = rel(2)))+stat_function(fun=dnorm, args=list(mean=mean(cumulative.score[,1]), sd=sd(cumulative.score[,1])))
#create frequency table of cumulative scores
write.csv((table(cumulative.score)),"freq.csv")
mean<-mean(cumulative.score[,1]) #mean of cumulative score
median<-median(cumulative.score[,1]) #median of cum score
sd<-sd(cumulative.score[,1])
#data_red<-data[,4:53]#reduce data to essential components
alpha_del<-reliability(score.matrix, itemal=TRUE) #obtain cronbach's alpha
write.csv(alpha_del$alpha.if.deleted,file="alpha_del.csv")
######Understanding Unidemensionality of Data #########################################
#Factor Analysis
factanal(score.matrix, factors=3) # varimax is the defaul
#PCA
summary(pc.cr <- princomp(score.matrix, cor = TRUE))
#Fleiss Kappa
kappam.fleiss(score.matrix, exact = FALSE, detail = FALSE)
cron_alpha<-alpha(score.matrix) #calculate cronbach alpha
cron_alpha
nrows<-nrow(score.matrix)# calculate number of rows
alpha.CI(cron_alpha,nrows, 50,level=0.95) # calculate CI for Cronbach alpha
discrim(score.matrix) #calculate discrimination for each item
summary(raw)# for table

write.csv(item.exam(score.matrix, discrim=TRUE),file="item.csv")
#Calculate quantiles
quantile(cumulative.score[,],probs=seq(0,1,0.10),type=3)

#Pass vs. Fail analysis
#Obtain people who pass
pass<-rep(0,nrow(score.matrix)) #create a pass column
total<-cbind(id,score.matrix,cumulative.score,pass)  #create a total data frame
total$pass[total$score>39]<-1 #assign everyone who as passed the test to 1
total$pass<-as.factor(total$pass) #set pass as a factor
summary(total) #summary
pass.mean<-mean(total$score[total$pass=="1"]) #mean of the passing people
pass.median<-median(total$score[total$pass=="1"]) #median of the passing people
fail.mean<-mean(total$score[total$pass=="0"]) #mean of failing people
fail.median<-median(total$score[total$pass=="0"])#median of failing people
pass.sd<-sd(total$score[total$pass=="1"]) #pass sd
fail.sd<-sd(total$score[total$pass=="0"])  #fail sd
score.pass<-total[total$pass=="1",]
score.fail<-total[total$pass=="0",]

pass.score<-score(score.pass[,2:51],output.scored=TRUE,rel=TRUE)
fail.score<-score(score.fail[,2:51],output.scored=TRUE,rel=TRUE) 
pass.rel<-pass.score$reliability
fail.rel<-fail.score$reliability
item.exam(total[,2:51],y=total[,53],discrim=TRUE)

write.csv(item.exam(score.pass[,2:51], discrim=TRUE),file="item_pass.csv")
write.csv(item.exam(score.fail[,2:51], discrim=TRUE),file="item_fail.csv")

t.test(score.pass$score, y = score.fail$score)
########### Item Response Theory##########################################

#Constrained Rasch Model
###fit <- rasch(score.matrix, constraint = cbind(ncol(score.matrix)+1, 1))
####summary(fit)
####coef(fit, prob = TRUE, order = TRUE)

#GoF.rasch(fit, B = 100)
#Unconstrained Rasch Model
fit_2 <- rasch(score.matrix,)
fit_2_sample<-rasch(sample,)
summary(fit_2)
write.csv(coef(fit_2, prob = TRUE, order = TRUE),file="rasch.csv")
GoF.rasch(fit_2, B = 20)
GoF.rasch(fit_2_sample,B=50)
fsc <- factor.scores(fit_2_sample)
plot(fsc, include.items = TRUE, main = "Kernel Density Estimate for Person Parameters")
legend("left", "item parameters", pch = 10, cex = 0.5, bty = "n")
#
#two parameter model
fit_3 <- ltm(score.matrix ~ z1)

#three parameter model
fit_5 <- tpm(score.matrix, type = "rasch", max.guessing = 1)
anova(fit2, fit4)

res.rasch <- RM(data[,4:53])
summary(res.rasch)
pres.rasch <- person.parameter(res.rasch)


#Factor Levels
factor.scores(fit_2)

#Graphing

plot(fit_2, legend = TRUE)
plot(fit_2, type = "IIC", annot = FALSE, lwd = 3, cex.main = 1.5,cex.lab = 1.3)
plot(fit_2, type = "IIC", items = 2, lwd = 3, cex.main = 1.5,cex.lab = 1.3)

info1 <- information(fit_2, c(-6, 0))
info2 <- information(fit_2, c(0, 6))
text(0.5, 0.5, labels = paste("Total Information:", round(info1$InfoTotal, 3),
                                 + "\n\nInformation in (-4, 0):", round(info1$InfoRange, 3),
                                 + paste("(", round(100 * info1$PropRange, 2), "%)", sep = ""),
                                 + "\n\nInformation in (0, 4):", round(info2$InfoRange, 3),
                                 + paste("(", round(100 * info2$PropRange, 2), "%)", sep = "")), cex = 1.5)

colnames(score.matrix)<-c("i1","i2","i3","i4","i5","i6","i7","i8","i9","i10","i11","i12","i13","i14","i15","i16","i17","i18","i19","i20","i21","i22","i23","i24","i25","i26","i27","i28","i29","i30","i31","i32","i33","i34","i35","i36","i37","i38","i39","i40","i41","i42","i43","i44","i45","i46","i47","i48","i49","i50")
#Goodness-of-fit for a Rasch model
res <- RM(score.matrix)
res<-RM(sample)
write.csv(coef(res), "summary_res.csv")
data_coef_res<-read.csv("summary_res.csv", header=TRUE)
#reduce matrix
score.matrix.red<-score.matrix[,-c(2,24,16,32,1,40,9,7,29,5,3,8)]
ggplot(data_coef_res,aes(x=diff)) + geom_histogram(binwidth=0.25, colour="black", fill="white")+labs(title="Advanced FHM Scores")+xlab("Total Score") +ylab("% of Total")+theme(plot.title = element_text(size = rel(2)))+stat_function(fun=dnorm, args=list(mean=mean(cumulative.score[,1]), sd=sd(cumulative.score[,1])))
#create frequency table of cumulative scores

plotPImap(res)

pres <- person.parameter(res)
lrt <- LRtest(res, splitcr="median",se = TRUE)
lrt
plotGOF(lrt,beta.subset=c(1:50), tlab="item", conf=list(ia=FALSE,col="blue",lty=2))
pp <- person.parameter(res)
plot(pp)
plotGOF(lrt, conf = list())
itemfit(pp)
t11 <- NPtest(score.matrix, method = "T11")
pcm<-PCM(score.matrix)
gof.res <- gofIRT(pres)
gof.res
summary(gof.res)


library(mirt)
(mod1 <- mirt(data_red, 1))
(mod2 <- mirt(data_red, 2))
frequenc_res<-residuals(mod1, restype = "exp")
write.table(frequenc_res, file="freq.csv", sep=",", col.names=TRUE)
